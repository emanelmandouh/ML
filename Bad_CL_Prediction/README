README 
- This file describes a Classification ML Approach to predict bad code commits 
- ML model will be used to predict for active development work, if the upcoming code changes are risky or not (will results in regression failure)
- This will save huge amount of time in the development cycle and reduce the TAT between bug injection and bug detection in the SW development cycle. By 
	- Prevent the check-in until the author assures all tests pass locally before code integration to product branch.
	- Automate localize root cause of regression test failure : once the regression test fails, the list of code commits between first fail and last pass will be provided to the predictor to identify the culprit bad commits
          and help in identify root cause of the failure of regression test. 
-------------------------------------------------------------------------------------------------------------------------------------------
Step1: Data Loading and Exploration 
-------------------------------------------------------------------------------------------------------------------------------------------
- This project use data from 4 Open-HW Platforms Namely CVA6, Open Titian, IBEX
- It Extracts the Code Changes/Commits for Github Reprository , since it is open source so Github Data is accessible from 
	- Open Titan : https://github.com/lowRISC/opentitan
	- Open Ibex : https://github.com/lowRISC/ibex
	- CVA6 : https://github.com/openhwgroup/cva6
- Our Own , Extractor is used to extract from github metadata , list of features we beleiev may contribute in predicting the good/bad code commits as refered in many publications about same topic.
- Original Data Size is ( 28854 entries, 185 Coloumns Size of 40 MB)
-------------------------------------------------------------------------------------------------------------------------------------------
- Step2: Data Understanding , here we concluded 
-------------------------------------------------------------------------------------------------------------------------------------------
  - 
  - The Traget Variable 'is_bad' describes if the code commits are bad commit (resuled in test failure) or good commit (has no test failure associated).
  - The extracted data covers up tp 185 Features and No of 28K data points) --> accessible from ./data/Code-Commits.csv 
  - Critical Note : Raw Data should not be shared in public as it reveal features list identification and extractor data 
  - Data Processing : Cover Exclusion of Some Feature Variations (for same feature we have aggregated values _avg,_mean, _max,_mode,..) so deceison has been made to select "max" as most appropiate aggregation 
  - Data Processing : So Initial Features for Exploraton and Processing : are ~ 40 Features as Listed 
	- Data Reading & Feature List
	- Project : Project ID Titan, Ibex, CVA6, In-House (1,2,3,4)
	- id : Code Commit CL Number
	- author : Author of Code Change
	- commit_message : github Commit Message
	- hour_of_day : Hour of Day when code commit submitted
	- day_of_week : Day of the Week when code commit submitted
	- week_of_month : Week of the Month when code commit submitted
	- month_of_year : Month of Year wehn code commit submitted
	- is_fix : Was the Code Check-Ins for Bug Fix?
	- num_affected_files : No of Filed Affected by the Change
	- num_affected_dirs : No of Affected Directories by the Change
	- num_rtl_files : No of RTL related Files
	- num_files_added : Files Added
	- num_files_deleted : Filed Deleted
	- num_lines_added : Line added
	- num_lines_deleted : Line deleted
	- comments_added : No of Comment Lines Added
	- comments_deleted : No of Comment Lines Deleted
	- blank_lines_added : No of Blank Lines Added
	- blank_lines_deleted : No of Blank Lines Deleted
	- type_changed : Type of Change : Source Code, Documentation, Others
	- file_complexity_max : Complexity of Changed File
	- nested_cond_file_max : Nested Conditions in the Changed File
	- nested_loops_file_max : Nested Loops in the Changed File
	- commits_per_file_max : No of Commits per File (as Indictaion for amount of changable Files )
	- failures_per_file_max : No of Bad Commits Submitted for this File
	- ext_modules_max : External Modules (No of Instantiated Modules)
	- nested_cond_add_max : Added Code Complexity ( Conditions)
	- nested_loops_add_max : Added Code Complexity (Nested Loops)
	- import_modules_max : No of Included Files in Changed File
	- avg_nest_lvl_cond_added_max : Added Code Compelxity (Level of Nested Conditions)
	- avg_nest_lvl_loop_added_max : Added Code Complexity (Level of Nested Loops)
	- complexity_added_max : Added Code Cyclomatic Complexity
	- avg_nest_lvl_cond_file_max : File Complexity (Level of Nested Conditions)
	- avg_nest_lvl_loop_file_max : File Complexity (Level of Nested Loop)
	- author_commits_on_files_max : Author Experience (No of Commits he did in the File)
	- author_failed_commits_on_files_max : Author Experience (No of Bad Commits he did in File)
	- is_bad : Target Variable (Was this Bad Commit or Not )
  - The Data has unbalnced Classes with majority of "No" : Ratio of Bad Commits vs all data points are ~21% 
  - Data Demonstrates The different features show that they differ in the range of values. When modeling, standardization is required
-------------------------------------------------------------------------------------------------------------------------------------------
- Step3: Data Mainpulation
-------------------------------------------------------------------------------------------------------------------------------------------
  - Some Fileds Exclusion : Commit -ID String, Commiy-Message 
  - Categoroical Data Encoding : Project ID , is_fix --> Label Encoding / will explore target encoding
  - Change Type : Has been encoded as Multi Binary Encoding (Source, Test, Confid, Other, Doc) if Field of "Type_changed" contained any, 1 
  - Check null values, when we found many data points have empty entries for following list, this because if the change is not related to source rtl files then these metrices are empty or not extracted.
	nested_cond_file_max                  2577
	ext_modules_max                       2577
	file_complexity_max                   2577
	import_modules_max                    2577
	avg_nest_lvl_cond_file_max            2577
	avg_nest_lvl_loop_file_max            2577
	nested_loops_file_max                 2577
  - Numerical Value Standarization (Scaler)
  - Train & Test Data Splitting with 30-70%
-------------------------------------------------------------------------------------------------------------------------------------------
- Step4: Study Correlation between Numerical Variables , Categorical Variables w.r.t Output Variable "is_bad", we concluded 
  - Outliers : Histogram has been drwan to understand the distribution of all numeric values , some outliers discovered --> further work in handling outliers should be done.
  - Project 4, has bad code check-ins vs project 1,which reflect more about complexity and large scope of proj4 vs proj1
  - Bad commit ratios in code changes that are not for bug fixes came higher than code changes for bug fixes, which contradict with some assumption about bug fixes changes may inroduce more bugs.
  - Highest bad commit ratios are related to code changes in source, with less failures happen in other changes like docs/config/..
  - Histogram of Numerical Features Distribution mainfest outliesr in many such as : num of affected files, dirs, added files, deleted files,... so outliers mainpulation must be handled (Work in Progress)
  - 
-------------------------------------------------------------------------------------------------------------------------------------------
- Step5 : Modeling/Evluation --> Work In Progress for Feature Selection, Splitting and Hyper Parameters, Model Selections.
-------------------------------------------------------------------------------------------------------------------------------------------

  - Step5.1 : Simple Models Fitting
	- Simple Logistic Regression, KNN, Decesion Tree and SVN Models have been created with the following Details
		- [LR] Cross Validation Accuarcy Score: 0.7693 +/- 0.024 (std) min: 0.7269, max: 0.8102
		- [KNN] Cross Validation Accuarcy Score: 0.755 +/- 0.0266 (std) min: 0.7083, max: 0.7778
		- [SVM] Cross Validation Accuarcy Score: 0.7612 +/- 0.0276 (std) min: 0.7269, max: 0.8148
		- [Decision Tree] Cross Validation Accuarcy Score: 0.7729 +/- 0.0329 (std) min: 0.7778, max: 0.8333
	- Then use Stratified Kfold to Avoid UnBalance in Classes 
		- [LR] Cross Validation Accuarcy Score: 0.7684 +/- 0.0137 (std) min: 0.7535, max: 0.7963
		- [KNN] Cross Validation Accuarcy Score: 0.7543 +/- 0.0186 (std) min: 0.7222, max: 0.7546
		- [SVM] Cross Validation Accuarcy Score: 0.7607 +/- 0.0178 (std) min: 0.7685, max: 0.7778
		- [Decision Tree] Cross Validation Accuarcy Score: 0.7706 +/- 0.0261 (std) min: 0.7546, max: 0.8333
  - Step 5.2 : Looking for Best Model Parameters Using GridSearch
	- To Explore Best Model Paramters we ran group of parameters, kernel variations to excercise the data with 
	- KNN (NN Ranges from 1 to 10) --> Best Model 'model__n_neighbors': 8 , accuracy 0.771 and Fit Time 6.7877631187438965
	- LR (C=[0.001, 0.01, 0.1, 1, 10, 100, 1000]) --> Best Model 'model__C': 1, accuracy 0.777, Fit Time 105.25439095497131
	- SVM (kernel=['rbf'], model__gamma=[1e-3,1e-4, 1e-5],model__C=[ 1, 10, 100]) --> {'model__C': 1, 'model__gamma': 0.0001, 'model__kernel': 'rbf'} with accuracy 0.779 and Fit Time of 53.450621366500854
	- DT(criterion = ['gini','entropy'],model__max_depth = [5, 10, 15, 20, 25, 30]) --> Best Model 'model__criterion': 'entropy', 'model__max_depth': 5 , acuracy of 0.83, Fit Time : 3.375972032546997
-------------------------------------------------------------------------------------------------------------------------------------------
-Step6: Deployment /Recommendation
-------------------------------------------------------------------------------------------------------------------------------------------
- Create Default 4 Classifiers using Cross Validation for Data Splitting Resulted in : LR(0.8102), KNN(0.7778), SVM(0.8148), DT(0.8333) i.e some models came with accuracy less than Base 
- So we utilized Stratified Kfold to handle unblanced Class Resulted in No Major Changes LR(0.7963), KNN(0.7546), SVM(0.7778), DT(0.8333)
- GridSearch has been used to obtain best KNN, LR, SVM, DT Models Looking across multiple Hyper Parameters Results Demonstrates 
- Step6: GridSearch and Model Hyper Parameters 
-------------------------------------------------------------------------------------------------------------------------------------------
Model	Train Time	Train Accuracy	Test Accuracy
0	KNN	6.787763	0.792852	0.760802
1	LR	105.254391	0.788882	0.762346
2	SVN	53.450621	0.802780	0.771605
3	Decesion-Tree	3.375972	0.855063	0.802469
-------------------------------------------------------------------------------------------------------------------------------------------
 
